{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비 및 공통 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from dataloader.dataloader import data_loader\n",
    "from utils.data_split import data_split\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "dataset_name = \"pure\"\n",
    "train_df, _, _, _, target_column = data_loader(dataset_name)\n",
    "X_train, X_valid, y_train, y_valid = data_split(\"time\", train_df, target_column)\n",
    "\n",
    "def update_yaml(file_path, new_params, param_key='reg_params'):\n",
    "    with open(file_path, 'r') as f:\n",
    "        params = yaml.safe_load(f)\n",
    "    \n",
    "    params[param_key].update(new_params)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        yaml.dump(params, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  XGBoost 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | max_depth | n_esti... |\n",
      "-------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-1.811e+0\u001b[39m | \u001b[39m0.4553   \u001b[39m | \u001b[39m0.02114  \u001b[39m | \u001b[39m7.209    \u001b[39m | \u001b[39m419.2    \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-2.337e+0\u001b[39m | \u001b[39m0.3275   \u001b[39m | \u001b[39m0.2451   \u001b[39m | \u001b[39m6.453    \u001b[39m | \u001b[39m478.6    \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m-1.551e+0\u001b[39m | \u001b[35m0.4553   \u001b[39m | \u001b[35m0.1516   \u001b[39m | \u001b[35m8.515    \u001b[39m | \u001b[35m419.2    \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-1.569e+0\u001b[39m | \u001b[39m0.6782   \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m420.5    \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-1.724e+0\u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m416.7    \u001b[39m |\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-3.373e+0\u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.01     \u001b[39m | \u001b[39m9.648    \u001b[39m | \u001b[39m424.7    \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-2.385e+0\u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m6.753    \u001b[39m | \u001b[39m410.2    \u001b[39m |\n",
      "=========================================================================\n",
      "XGBoost best parameters: {'max_depth': 8, 'learning_rate': 0.15161899093869716, 'n_estimators': 419, 'colsample_bytree': 0.45526618254159534}\n"
     ]
    }
   ],
   "source": [
    "def xgb_evaluate(max_depth, learning_rate, n_estimators, colsample_bytree):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'device': 'cuda'\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=3).mean()\n",
    "\n",
    "xgb_bo = BayesianOptimization(\n",
    "    xgb_evaluate,\n",
    "    {\n",
    "        'max_depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'n_estimators': (50, 500),\n",
    "        'colsample_bytree': (0.3, 1.0)\n",
    "    }\n",
    ")\n",
    "xgb_bo.maximize(n_iter=50, init_points=5)\n",
    "\n",
    "# 파라미터 업데이트\n",
    "best_xgb_params = {\n",
    "    'max_depth': int(xgb_bo.max['params']['max_depth']),\n",
    "    'learning_rate': float(xgb_bo.max['params']['learning_rate']),\n",
    "    'n_estimators': int(xgb_bo.max['params']['n_estimators']),\n",
    "    'colsample_bytree': float(xgb_bo.max['params']['colsample_bytree'])\n",
    "}\n",
    "\n",
    "# YAML 파일 읽기\n",
    "with open('models/params/xgb_param.yaml', 'r') as f:\n",
    "    yaml_content = f.read()\n",
    "\n",
    "# 최적화된 파라미터 업데이트\n",
    "for param, value in best_xgb_params.items():\n",
    "    pattern = rf\"{param}:.*\"\n",
    "    replacement = f\"{param}: {value}\"\n",
    "    yaml_content = re.sub(pattern, replacement, yaml_content)\n",
    "\n",
    "# 업데이트된 내용을 YAML 파일에 쓰기\n",
    "with open('models/params/xgb_param.yaml', 'w') as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(\"XGBoost best parameters:\", best_xgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | colsam... | learni... | n_esti... | num_le... |\n",
      "-------------------------------------------------------------------------\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005926 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-1.469e+0\u001b[39m | \u001b[39m0.6546   \u001b[39m | \u001b[39m0.1466   \u001b[39m | \u001b[39m134.3    \u001b[39m | \u001b[39m35.97    \u001b[39m |\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004031 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004118 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004390 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-1.807e+0\u001b[39m | \u001b[39m0.3302   \u001b[39m | \u001b[39m0.2284   \u001b[39m | \u001b[39m244.0    \u001b[39m | \u001b[39m99.09    \u001b[39m |\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007284 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005972 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005866 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m3        \u001b[39m | \u001b[39m-1.532e+0\u001b[39m | \u001b[39m0.6907   \u001b[39m | \u001b[39m0.1026   \u001b[39m | \u001b[39m134.7    \u001b[39m | \u001b[39m34.72    \u001b[39m |\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005011 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005070 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005129 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-1.658e+0\u001b[39m | \u001b[39m0.5278   \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m132.9    \u001b[39m | \u001b[39m40.67    \u001b[39m |\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004053 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004152 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-1.9e+08 \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m136.9    \u001b[39m | \u001b[39m37.38    \u001b[39m |\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005840 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005922 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006114 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m6        \u001b[39m | \u001b[39m-1.582e+0\u001b[39m | \u001b[39m0.824    \u001b[39m | \u001b[39m0.07552  \u001b[39m | \u001b[39m133.1    \u001b[39m | \u001b[39m35.38    \u001b[39m |\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 41478.040728\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004123 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1228\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 37498.813959\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004253 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1223\n",
      "[LightGBM] [Info] Number of data points in the train set: 1062908, number of used features: 9\n",
      "[LightGBM] [Info] Start training from score 34467.345097\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-1.903e+0\u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m0.3      \u001b[39m | \u001b[39m133.8    \u001b[39m | \u001b[39m32.64    \u001b[39m |\n",
      "=========================================================================\n",
      "LightGBM best parameters: {'num_leaves': 35, 'learning_rate': 0.1466152700102118, 'n_estimators': 134, 'colsample_bytree': 0.6546472576714326}\n"
     ]
    }
   ],
   "source": [
    "def lgb_evaluate(num_leaves, learning_rate, n_estimators, colsample_bytree):\n",
    "    params = {\n",
    "        'num_leaves': int(num_leaves),\n",
    "        'learning_rate': learning_rate,\n",
    "        'n_estimators': int(n_estimators),\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'device': 'cpu'  \n",
    "    }\n",
    "    model = LGBMRegressor(**params)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=3).mean()\n",
    "\n",
    "lgb_bo = BayesianOptimization(\n",
    "    lgb_evaluate,\n",
    "    {\n",
    "        'num_leaves': (20, 100),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'n_estimators': (50, 500),\n",
    "        'colsample_bytree': (0.3, 1.0)\n",
    "    }\n",
    ")\n",
    "lgb_bo.maximize(n_iter=50, init_points=5)\n",
    "\n",
    "# 파라미터 업데이트\n",
    "best_lgb_params = {\n",
    "    'num_leaves': int(lgb_bo.max['params']['num_leaves']),\n",
    "    'learning_rate': float(lgb_bo.max['params']['learning_rate']),\n",
    "    'n_estimators': int(lgb_bo.max['params']['n_estimators']),\n",
    "    'colsample_bytree': float(lgb_bo.max['params']['colsample_bytree'])\n",
    "}\n",
    "\n",
    "# YAML 파일 읽기\n",
    "yaml_file_path = 'models/params/lgbm_param.yaml'\n",
    "try:\n",
    "    with open(yaml_file_path, 'r') as f:\n",
    "        yaml_content = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Warning: {yaml_file_path} not found. Creating a new file.\")\n",
    "    yaml_content = {'reg_params': {}, 'cls_params': {}}\n",
    "\n",
    "# 최적화된 파라미터 업데이트\n",
    "if 'reg_params' not in yaml_content:\n",
    "    yaml_content['reg_params'] = {}\n",
    "yaml_content['reg_params'].update(best_lgb_params)\n",
    "yaml_content['reg_params']['device'] = 'cpu' \n",
    "\n",
    "# 업데이트된 내용을 YAML 파일에 쓰기\n",
    "with open(yaml_file_path, 'w') as f:\n",
    "    yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "\n",
    "print(\"LightGBM best parameters:\", best_lgb_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost 최적화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | baggin... |   depth   | iterat... | l2_lea... | learni... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[39m1        \u001b[39m | \u001b[39m-1.41e+08\u001b[39m | \u001b[39m0.3745   \u001b[39m | \u001b[39m9.655    \u001b[39m | \u001b[39m379.4    \u001b[39m | \u001b[39m6.388    \u001b[39m | \u001b[39m0.05525  \u001b[39m |\n",
      "| \u001b[39m2        \u001b[39m | \u001b[39m-1.835e+0\u001b[39m | \u001b[39m0.156    \u001b[39m | \u001b[39m3.407    \u001b[39m | \u001b[39m439.8    \u001b[39m | \u001b[39m6.41     \u001b[39m | \u001b[39m0.2153   \u001b[39m |\n",
      "| \u001b[35m3        \u001b[39m | \u001b[35m-1.358e+0\u001b[39m | \u001b[35m0.0592   \u001b[39m | \u001b[35m8.966    \u001b[39m | \u001b[35m378.8    \u001b[39m | \u001b[35m7.281    \u001b[39m | \u001b[35m0.1084   \u001b[39m |\n",
      "| \u001b[39m4        \u001b[39m | \u001b[39m-1.736e+0\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m3.0      \u001b[39m | \u001b[39m372.3    \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[39m5        \u001b[39m | \u001b[39m-1.505e+0\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m6.842    \u001b[39m | \u001b[39m382.5    \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m0.3      \u001b[39m |\n",
      "| \u001b[35m6        \u001b[39m | \u001b[35m-1.282e+0\u001b[39m | \u001b[35m0.0      \u001b[39m | \u001b[35m10.0     \u001b[39m | \u001b[35m374.3    \u001b[39m | \u001b[35m5.826    \u001b[39m | \u001b[35m0.3      \u001b[39m |\n",
      "| \u001b[39m7        \u001b[39m | \u001b[39m-2.128e+0\u001b[39m | \u001b[39m0.0      \u001b[39m | \u001b[39m10.0     \u001b[39m | \u001b[39m371.2    \u001b[39m | \u001b[39m1.0      \u001b[39m | \u001b[39m0.01     \u001b[39m |\n",
      "=====================================================================================\n",
      "CatBoost best parameters: {'depth': 10, 'learning_rate': 0.3, 'iterations': 374, 'l2_leaf_reg': 5.825579340588563, 'bagging_temperature': 0.0}\n",
      "Updated YAML content:\n",
      "cls_params:\n",
      "  bagging_temperature: 1\n",
      "  cat_features: []\n",
      "  depth: 6\n",
      "  devices: '0'\n",
      "  eval_metric: AUC\n",
      "  iterations: 1000\n",
      "  l2_leaf_reg: 3\n",
      "  learning_rate: 0.1\n",
      "  loss_function: MultiClass\n",
      "  random_seed: 42\n",
      "  task_type: GPU\n",
      "  verbose: 100\n",
      "reg_params:\n",
      "  bagging_temperature: 0.0\n",
      "  border_count: 254\n",
      "  depth: 10\n",
      "  devices: '0'\n",
      "  iterations: 374\n",
      "  l2_leaf_reg: 5.825579340588563\n",
      "  learning_rate: 0.3\n",
      "  loss_function: RMSE\n",
      "  model_size_reg: 0.5\n",
      "  od_type: Iter\n",
      "  od_wait: 20\n",
      "  random_seed: 0\n",
      "  rsm: 1.0\n",
      "  task_type: GPU\n",
      "  thread_count: -1\n",
      "  verbose: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cat_evaluate(depth, learning_rate, iterations, l2_leaf_reg, bagging_temperature):\n",
    "    params = {\n",
    "        'depth': int(depth),\n",
    "        'learning_rate': learning_rate,\n",
    "        'iterations': int(iterations),\n",
    "        'l2_leaf_reg': l2_leaf_reg,\n",
    "        'bagging_temperature': bagging_temperature,\n",
    "        'task_type': 'GPU'\n",
    "    }\n",
    "    model = CatBoostRegressor(**params, verbose=False)\n",
    "    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=3).mean()\n",
    "\n",
    "cat_bo = BayesianOptimization(\n",
    "    cat_evaluate,\n",
    "    {\n",
    "        'depth': (3, 10),\n",
    "        'learning_rate': (0.01, 0.3),\n",
    "        'iterations': (50, 500),\n",
    "        'l2_leaf_reg': (1, 10),\n",
    "        'bagging_temperature': (0, 1)\n",
    "    },\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "try:\n",
    "    cat_bo.maximize(n_iter=50, init_points=5)\n",
    "\n",
    "    # 파라미터 업데이트\n",
    "    best_cat_params = {\n",
    "        'depth': int(cat_bo.max['params']['depth']),\n",
    "        'learning_rate': float(cat_bo.max['params']['learning_rate']),\n",
    "        'iterations': int(cat_bo.max['params']['iterations']),\n",
    "        'l2_leaf_reg': float(cat_bo.max['params']['l2_leaf_reg']),\n",
    "        'bagging_temperature': float(cat_bo.max['params']['bagging_temperature'])\n",
    "    }\n",
    "\n",
    "    # YAML 파일 읽기\n",
    "    yaml_file_path = 'models/params/catboost_param.yaml'\n",
    "    with open(yaml_file_path, 'r') as f:\n",
    "        existing_params = yaml.safe_load(f)\n",
    "\n",
    "    # 최적화된 파라미터 업데이트\n",
    "    for key, value in best_cat_params.items():\n",
    "        if key in existing_params['reg_params']:\n",
    "            existing_params['reg_params'][key] = value\n",
    "\n",
    "    # 업데이트된 내용을 YAML 파일에 쓰기\n",
    "    with open(yaml_file_path, 'w') as f:\n",
    "        yaml.dump(existing_params, f, default_flow_style=False)\n",
    "\n",
    "    print(\"CatBoost best parameters:\", best_cat_params)\n",
    "    print(\"Updated YAML content:\")\n",
    "    print(yaml.dump(existing_params, default_flow_style=False))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during optimization: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
